{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A fantasy styled overhead shot of two zebras running from a cheetah in the African savannah\"\n",
    "prompt = f\"\"\"I will provide you with a caption for a photo , image , or painting . Your task is to generate the bounding boxes for the objects mentioned in the caption , along with a background prompt describing the scene . The images are of size 512 x512 , and the bounding boxes should not overlap or go beyond the image boundaries . Each bounding box should be in the format of ( object name , [ top - left x coordinate , top - left y coordinate , box width , box height ]) and include exactly one object . The background prompt should not contain any objects but can add reasonable details, if necessary . Please refer to the example below for the desired format .\n",
    "\n",
    "Caption : A realistic image of four skiers standi\"ng in a line on the snow near a palm tree\n",
    "Objects : [ ('a skier ', [5 , 152 , 139 , 168]) , ('a skier ', [278 , 192 , 121 , 158]) , ('a skier ', [148 , 173 , 124 , 155]) , ('a palm tree ', [404 , 180 , 103 , 180]) ]\n",
    "Background prompt : A realistic image of an outdoor scene with snow\n",
    "\n",
    "Caption : Futuristic painting of a basketball player playing with a football in an indoor gym \n",
    "Objects : [ ('a basketball player ', [40 , 152 , 156 , 241]) , ('a football ', [237 , 17 , 54 , 32]) ]\n",
    "Background prompt : Futuristic painting of an indoor gym\n",
    "\n",
    "Caption : A pixelized image of a toy practicing his swordsmanship in a children's bedroom \n",
    "Objects : [ ('a toy ', [34 , 145 , 120 , 237]) ]\n",
    "Background prompt : A pixelized image of a children's bedroom\n",
    "\n",
    "Caption : An oil painting of dog swimming in a blue lake towards a large branch  \n",
    "Objects : [ ('a dog ', [208 , 201 , 164 , 89]) , ('a large branch ', [156 , 219 , 51 , 29]) ]\n",
    "Background prompt : An oil painting of a blue lake\n",
    "\n",
    "Caption : A low light image of a tractor in a field at sunset\n",
    "Objects : [ ('a tractor ', [246, 298, 165, 133]) ]\n",
    "Background prompt : A low light image of a field at sunset\n",
    "\n",
    "Caption : A surreal image of flower blooming in a pot\n",
    "Objects : [ ('flower ', [215, 162, 123, 163]), ('a pot ', [217, 411, 149, 90]) ]\n",
    "Background prompt :  A surreal image of the inside a house\n",
    "\n",
    "Caption : A watercolor painting of a couple of pandas eating bamboo in a forest\n",
    "Objects : [ ('a panda eating bamboo', [30, 133, 212, 226]), ('a panda eating bamboo', [262, 137, 222, 221]) ]\n",
    "Background prompt : A watercolor painting of a forest\n",
    "\n",
    "Caption : {text} \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned GPT2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"checkpoints_large/checkpoint2.pt/\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Objects: [ ('a cat', '[  1, 170, 125, 344]), ('a three dogs', '[  2,  80,  83, 264]')\n",
      "\n",
      "('a three dogs', '[240,  87, 512, 512]')\n",
      "\n",
      "('a three dogs', '[\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, top_p=0.9, top_k=50, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPT-1B-redpajama-200b-dolly](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivaen/.cache\\huggingface\\modules\\transformers_modules\\mosaicml\\mpt-1b-redpajama-200b-dolly\\d3586068c3d023c7fcfa3c7dbd3042b2f00db1e3\\attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mosaicml/mpt-1b-redpajama-200b-dolly\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-1b-redpajama-200b-dolly\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects: [ ('a cheetah ', [150, 211, 151, 141]), ('a zebra ', [155, 219, 57, 29]) ]\n",
      "Background prompt: A fantasy styled overhead shot of two zebras running from a cheetah in the African savannah<|endoftext|>Kansas City Chiefs quarterback\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, top_p=0.9, top_k=50, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RedPajama-INCITE-Instruct-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\", torch_dtype=torch.float16)\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects : [ ('a cheetah', [172, 219, 23, 221]), ('a zebra', [207, 202, 167, 182]), ('a zebra', [201, 211, 184, 212]), ('two zebras', [170, 197, 170, 227]) ]\n",
      "Background prompt : A fantasy styled overhead shot of two zebras running from a cheetah in the African savannah\n",
      "\n",
      "Caption : A man is sitting on a bench while a group of people are walking by on the street\n",
      "Objects : [ ('a man', [197, 196, 194, 209]), ('a bench',\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.8, top_k=100, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "print(output_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RedPajama-INCITE-7B-Instruct](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199bd1a0033f4c5f82244cebd75f2ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Instruct\", torch_dtype=torch.float16)\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects : [ ('a cheetah ', [20, 90, 34, 213]), ('two zebras ', [35, 142, 87, 225]) ]\n",
      "Background prompt : A fantasy styled overhead shot of two zebras running from a cheetah in the African savannah\n",
      "\n",
      "Caption : A photo of a person on a skateboard in a skate park \n",
      "Objects : [ ('a skateboarder ', [35, 115, 25, 144]) ]\n",
      "Background prompt : A photo of a person on a skateboard in a skate park\n",
      "\n",
      "Caption : A woman in a black and white dress\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=100, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LayoutSD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
